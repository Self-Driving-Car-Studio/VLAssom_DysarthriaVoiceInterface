import os
import torch
from dataclasses import dataclass
from typing import Any, Dict, List, Union
from datasets import Dataset, Audio
from transformers import (
    WhisperFeatureExtractor,
    WhisperTokenizer,
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)
from peft import LoraConfig, get_peft_model, TaskType
import torch


# ==========================================
# âš™ï¸ ì„¤ì • (ë‚´ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
# ==========================================
# ì‚¬ìš©í•  ê¸°ë³¸ ëª¨ë¸ (smallì´ ì„±ëŠ¥/ì†ë„ ë°¸ëŸ°ìŠ¤ê°€ ì¢‹ìŒ)
MODEL_ID = "openai/whisper-small"
# ë°ì´í„° ê²½ë¡œ
DATA_ROOT = "../dataset"
CSV_FILE = os.path.join(DATA_ROOT, "metadata.csv")
AUDIO_FOLDER = os.path.join(DATA_ROOT, "raw_audio")
# ì €ì¥í•  ê²½ë¡œ
OUTPUT_DIR = "../models/whisper-finetuned-v1"

# í•™ìŠµ ì„¤ì • (ë°ì´í„°ê°€ ì ìœ¼ë¯€ë¡œ ì§§ê²Œ ì„¤ì •ë¨)
MAX_STEPS = 300          # í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ (ë°ì´í„°ê°€ 50ê°œë©´ 300~500 ì¶”ì²œ)
BATCH_SIZE = 4           # í•œ ë²ˆì— í•™ìŠµí•  ë°ì´í„° ì–‘ (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ)
LEARNING_RATE = 1e-3     # í•™ìŠµë¥  (LoRAëŠ” ë³´í†µ 1e-3 ì‚¬ìš©)

# ==========================================
# 1. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
# ==========================================
print(f"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘... ({CSV_FILE})")

# CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ ì—ëŸ¬
if not os.path.exists(CSV_FILE):
    raise FileNotFoundError("metadata.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. 01_recorder.pyë¡œ ë°ì´í„°ë¥¼ ë¨¼ì € ë§Œë“œì„¸ìš”!")

# ë°ì´í„°ì…‹ ìƒì„±
dataset = Dataset.from_csv(CSV_FILE)

# ì˜¤ë””ì˜¤ ê²½ë¡œ ìˆ˜ì • (CSVì—ëŠ” íŒŒì¼ëª…ë§Œ ìˆìœ¼ë¯€ë¡œ ì „ì²´ ê²½ë¡œë¡œ ë³€ê²½)
def resolve_path(batch):
    batch["audio"] = os.path.join(AUDIO_FOLDER, batch["file_name"])
    return batch

dataset = dataset.map(resolve_path)
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))

print("âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!")

# ==========================================
# 2. í”„ë¡œì„¸ì„œ(Feature Extractor + Tokenizer) ì¤€ë¹„
# ==========================================
feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_ID)
tokenizer = WhisperTokenizer.from_pretrained(MODEL_ID, language="Korean", task="transcribe")
processor = WhisperProcessor.from_pretrained(MODEL_ID, language="Korean", task="transcribe")

def prepare_dataset(batch):
    # ì˜¤ë””ì˜¤ ë¡œë“œ ë° íŠ¹ì„± ì¶”ì¶œ
    audio = batch["audio"]
    batch["input_features"] = feature_extractor(audio["array"], sampling_rate=audio["sampling_rate"]).input_features[0]
    
    # í…ìŠ¤íŠ¸ë¥¼ ë¼ë²¨ IDë¡œ ë³€í™˜
    batch["labels"] = tokenizer(batch["sentence"]).input_ids
    return batch

print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)

# ==========================================
# 3. Data Collator (ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë„êµ¬)
# ==========================================
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # padding í† í°(-100) ì²˜ë¦¬ (ì†ì‹¤ ê³„ì‚° ì œì™¸ìš©)
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        
        # ì‹œì‘ í† í°ì´ ìˆìœ¼ë©´ ì˜ë¼ë‚´ê¸°
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

# ==========================================
# 4. ëª¨ë¸ ë¡œë“œ ë° LoRA ì„¤ì • (í•µì‹¬!)
# ==========================================
print(f"ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘... ({MODEL_ID})")
model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID, device_map="auto")

model.gradient_checkpointing_enable()
model.enable_input_require_grads()

# LoRA ì„¤ì • (ëª¨ë¸ ì „ì²´ë¥¼ í•™ìŠµí•˜ì§€ ì•Šê³  ì¼ë¶€ë§Œ í•™ìŠµ -> ë¹ ë¦„)
config = LoraConfig(
    r=32, 
    lora_alpha=64, 
    target_modules=["q_proj", "v_proj"], 
    lora_dropout=0.05, 
    bias="none",
    # task_type=TaskType.SEQ_2_SEQ_LM
)

model = get_peft_model(model, config)
model.config.use_cache = False
model.print_trainable_parameters() # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥



# ==========================================
# 5. í•™ìŠµ ì‹œì‘
# ==========================================
training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=1,
    learning_rate=LEARNING_RATE,
    max_steps=MAX_STEPS,
    gradient_checkpointing=True,
    fp16=True, # GPU ì§€ì› ì‹œ True, ì•„ë‹ˆë©´ False
    report_to="none",
    eval_strategy="no", # ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ í‰ê°€ ìƒëµ
    save_strategy="steps",
    save_steps=100,
    logging_steps=25,
    load_best_model_at_end=False,
)

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=dataset,
    data_collator=data_collator,
    # tokenizer=processor.feature_extractor,
)

print("\nğŸš€ í•™ìŠµ ì‹œì‘! (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...)")
trainer.train()

# ==========================================
# 6. ì €ì¥
# ==========================================
print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘... ({OUTPUT_DIR})")
model.save_pretrained(OUTPUT_DIR)
processor.save_pretrained(OUTPUT_DIR)
print("ğŸ‰ í•™ìŠµ ì™„ë£Œ! ì´ì œ 03_inference.pyë¥¼ ë§Œë“¤ì–´ ì‹¤í–‰í•´ë³´ì„¸ìš”.")
