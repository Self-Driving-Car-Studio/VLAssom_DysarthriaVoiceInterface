import os
import torch
import sounddevice as sd
import numpy as np
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from peft import PeftModel

# ==========================================
# âš™ï¸ ì„¤ì •
# ==========================================
# 1. ê¸°ë³¸ ëª¨ë¸ê³¼ í•™ìŠµëœ ì–´ëŒ‘í„°(LoRA) ê²½ë¡œ
BASE_MODEL = "openai/whisper-small"
ADAPTER_PATH = "../models/whisper-finetuned-v1"

# 2. ë…¹ìŒ ì„¤ì •
SR = 16000  # WhisperëŠ” ë¬´ì¡°ê±´ 16kHz
RECORD_SECONDS = 3  # í•œ ë²ˆì—ë“¤ì„ ì‹œê°„

# ==========================================
# 1. ëª¨ë¸ ë¡œë”© (í•™ìŠµëœ ê²°ê³¼ í•©ì¹˜ê¸°)
# ==========================================
print("â³ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤... (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦½ë‹ˆë‹¤)")

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ ì‹¤í–‰ ì¥ì¹˜: {device}")

# ê¸°ë³¸ í”„ë¡œì„¸ì„œ & ëª¨ë¸ ë¡œë“œ
processor = WhisperProcessor.from_pretrained(BASE_MODEL, language="Korean", task="transcribe")
model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL, device_map=device)

# ğŸŒŸ í•µì‹¬: ë‚´ê°€ í•™ìŠµì‹œí‚¨ LoRA ì–´ëŒ‘í„°ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ì¥ì°©!
if os.path.exists(ADAPTER_PATH):
    model = PeftModel.from_pretrained(model, ADAPTER_PATH)
    print("âœ… í•™ìŠµëœ ë§ì¶¤í˜• ëª¨ë¸(LoRA)ì´ ì„±ê³µì ìœ¼ë¡œ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
else:
    print("âš ï¸ ê²½ê³ : í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")

# ==========================================
# 2. ì¶”ë¡  ë° ë¡œë´‡ ì œì–´ í•¨ìˆ˜
# ==========================================
def robot_action(text):
    """ì¸ì‹ëœ í…ìŠ¤íŠ¸ì— ë”°ë¼ ë¡œë´‡ ë™ì‘ì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜"""
    print(f"\nğŸ¤– [ì¸ì‹ ê²°ê³¼]: '{text}'")
    
    if "ë¹„íƒ€ë¯¼" in text and "ì¤˜" in text:
        print("   â””â”€ ğŸ¦¾ ë™ì‘: VLAssomì´ ë¹„íƒ€ë¯¼ì„ ì¡ì•„ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    elif "íƒ€ì´ë ˆë†€" in text and "ì¤˜" in text :
        print("   â””â”€ ğŸ¦¾ ë™ì‘: VLAssomì´ íƒ€ì´ë ˆë†€ì„ ì¡ì•„ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    elif "ì—°í•„" in text and "ì¤˜" in text:
        print("   â””â”€ ğŸ¦¾ ë™ì‘: VLAssomì´ ì—°í•„ì„ ì¡ì•„ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    else:
        print("   â””â”€ â“ ë™ì‘: (ì •ì˜ë˜ì§€ ì•Šì€ ëª…ë ¹ì–´ì…ë‹ˆë‹¤)")

def transcribe_audio(audio_data):
    """ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    # 1. ì „ì²˜ë¦¬
    input_features = processor(
        audio_data, 
        sampling_rate=SR, 
        return_tensors="pt"
    ).input_features.to(device)

    # 2. ì¶”ë¡  (ìƒì„±)
    with torch.no_grad():
        generated_ids = model.generate(input_features, language="korean")

    # 3. ë””ì½”ë”© (ìˆ«ì -> ê¸€ì)
    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return transcription.strip()

# ==========================================
# 3. ë©”ì¸ ì‹¤í–‰ ë£¨í”„
# ==========================================
print("\n" + "="*50)
print("ğŸ¤ VLAssom ìŒì„± ì œì–´ ì¸í„°í˜ì´ìŠ¤ ì‹œì‘")
print("="*50)

try:
    while True:
        input("\nâŒ¨ï¸ ì—”í„°(Enter)ë¥¼ ëˆ„ë¥´ë©´ 3ì´ˆê°„ ë“£ìŠµë‹ˆë‹¤... (ì¢…ë£Œ: Ctrl+C)")
        
        # 1. ë…¹ìŒ
        print("ğŸ”´ ë“£ê³  ìˆìŠµë‹ˆë‹¤...")
        recording = sd.rec(int(RECORD_SECONDS * SR), samplerate=SR, channels=1)
        sd.wait()
        print("âœ… ì²˜ë¦¬ ì¤‘...")

        # 2. ì°¨ì› ë³€í™˜ (Whisper ì…ë ¥ ê·œê²©ì— ë§ì¶¤)
        audio_data = recording.flatten()

        # 3. í…ìŠ¤íŠ¸ ë³€í™˜
        result_text = transcribe_audio(audio_data)

        # 4. ë¡œë´‡ ë™ì‘ ì‹¤í–‰
        robot_action(result_text)

except KeyboardInterrupt:
    print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
